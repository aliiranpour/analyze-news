from langchain_core.prompts import ChatPromptTemplate
from models.news_types import NewsState
from utils.logger import get_logger
from utils.llm import get_llm
from utils.retry import call_chain_with_backoff 
from utils.setting import BATCH_SIZE 

llm = get_llm()
logger = get_logger(__name__)

def summarize_article(content: str) -> str:
    """ØªÙˆÙ„ÛŒØ¯ Ø®Ù„Ø§ØµÙ‡â€ŒØ§ÛŒ Ø¯Ùˆ Ø¬Ù…Ù„Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø®Ø¨Ø± Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ Ø¨ÙˆØ±Ø³ÛŒ"""

    prompt = ChatPromptTemplate.from_template(
        """
        Ø´Ù…Ø§ ÛŒÚ© ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¯Ø± Ø­ÙˆØ²Ù‡ Ø§Ù‚ØªØµØ§Ø¯ØŒ Ø¨Ø§Ø²Ø§Ø± Ø³Ø±Ù…Ø§ÛŒÙ‡ØŒ Ø§Ø±Ø² Ø¯ÛŒØ¬ÛŒØªØ§Ù„ØŒ Ø·Ù„Ø§ØŒ Ø¯Ù„Ø§Ø± Ùˆ Ù…Ø³Ø§Ø¦Ù„ Ú©Ù„Ø§Ù† Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ù‡Ø³ØªÛŒØ¯.

        ÙˆØ¸ÛŒÙÙ‡ Ø´Ù…Ø§ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø®Ø¨Ø±ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…Ø¯Ù‡. Ø®Ù„Ø§ØµÙ‡ Ø¨Ø§ÛŒØ¯ Ø­Ø¯Ø§Ú©Ø«Ø± **Ø¯Ùˆ Ø¬Ù…Ù„Ù‡** Ø¨Ø§Ø´Ø¯ Ùˆ ÙÙ‚Ø· Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ùˆ ØªØ£Ø«ÛŒØ±Ú¯Ø°Ø§Ø±ØªØ±ÛŒÙ† Ù†Ú©Ø§Øª Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø±Ø§ Ù¾ÙˆØ´Ø´ Ø¯Ù‡Ø¯.

        ğŸŸ¡ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø¯Ø± Ø§ÙˆÙ„ÙˆÛŒØª ØªÙˆØ¬Ù‡ Ø´Ù…Ø§ Ù‡Ø³ØªÙ†Ø¯:
        - Ù‡Ø± Ù†ÙˆØ¹ Ø§Ø´Ø§Ø±Ù‡ Ø¨Ù‡ Ø¨Ø§Ø²Ø§Ø± Ø¨ÙˆØ±Ø³ØŒ Ø¯Ù„Ø§Ø±ØŒ Ø·Ù„Ø§ØŒ Ø§Ø±Ø² Ø¯ÛŒØ¬ÛŒØªØ§Ù„
        - ÙˆØ§Ú˜Ú¯Ø§Ù† Ùˆ Ù…ÙØ§Ù‡ÛŒÙ…ÛŒ Ù…Ø§Ù†Ù†Ø¯: Ø¬Ù†Ú¯ØŒ ØªØ­Ø±ÛŒÙ…ØŒ ÙˆØ±Ø´Ú©Ø³ØªÚ¯ÛŒØŒ Ø²ÛŒØ§Ù†ØŒ Ø³ÙˆØ¯ØŒ ØªÙˆØ±Ù…ØŒ Ø±Ú©ÙˆØ¯ØŒ ØµØ§Ø¯Ø±Ø§ØªØŒ ÙˆØ§Ø±Ø¯Ø§ØªØŒ Ú©Ø³Ø±ÛŒ Ø¨ÙˆØ¯Ø¬Ù‡ØŒ Ø¨Ù‡Ø±Ù‡ Ø¨Ø§Ù†Ú©ÛŒØŒ Ù†Ø±Ø® Ø§Ø±Ø²ØŒ Ø³ÛŒØ§Ø³Øª Ù¾ÙˆÙ„ÛŒØŒ Ø³ÛŒØ§Ø³Øª Ù…Ø§Ù„ÛŒØŒ ØªØµÙ…ÛŒÙ…Ø§Øª Ø¨Ø§Ù†Ú© Ù…Ø±Ú©Ø²ÛŒØŒ Ø¨Ø­Ø±Ø§Ù† Ù…Ø§Ù„ÛŒ Ùˆ Ø³ÛŒØ§Ø³ÛŒ
        - Ù‡Ø± Ú¯ÙˆÙ†Ù‡ Ø§Ø´Ø§Ø±Ù‡ Ø¨Ù‡ Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ ÛŒØ§ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ø¨ÙˆØ±Ø³ÛŒ (Ø¯Ø± Ø§ÛŒÙ† ØµÙˆØ±ØªØŒ Ø­ØªÙ…Ø§Ù‹ Ù†Ø§Ù… Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø± Ø®Ù„Ø§ØµÙ‡ Ø¨ÛŒØ§ÙˆØ±)
        - Ø§Ø®Ø¨Ø§Ø±ÛŒ Ú©Ù‡ Ø¨Ø± Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ Ø§ÛŒØ±Ø§Ù† ÛŒØ§ Ø¬Ù‡Ø§Ù† ØªØ£Ø«ÛŒØ±Ú¯Ø°Ø§Ø± Ù‡Ø³ØªÙ†Ø¯

        ğŸ”´ ØªÙˆØ¬Ù‡:
        Ø§Ú¯Ø± Ù…ØªÙ† Ø®Ø¨Ø± **ÙØ§Ù‚Ø¯ Ù…Ø­ØªÙˆØ§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ ÛŒØ§ Ù‚Ø§Ø¨Ù„ ØªØ­Ù„ÛŒÙ„ Ø¨Ø§Ø´Ø¯** (Ù…Ø«Ù„Ø§Ù‹ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª ÛŒØ§ ÙÙ‚Ø· ØªÛŒØªØ± ØªÚ©Ø±Ø§Ø±ÛŒ Ø§Ø³Øª)ØŒ ÙÙ‚Ø· Ø¨Ù†ÙˆÛŒØ³:
        Â«Ø®Ø¨Ø± ÙØ§Ù‚Ø¯ Ù…Ø­ØªÙˆØ§ÛŒ Ù‚Ø§Ø¨Ù„ ØªØ­Ù„ÛŒÙ„ Ø§Ø³Øª.Â»

        Ø­Ø§Ù„ Ø®Ø¨Ø± Ø²ÛŒØ± Ø±Ø§ Ø¨Ø§ Ø¯Ù‚Øª Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø³Ù¾Ø³ Ø®Ù„Ø§ØµÙ‡ Ú©Ù†:

        Ø®Ø¨Ø±:
        {content}
        """
    )
    chain = prompt | llm
    response = call_chain_with_backoff(chain, {'content': content})
    return response.content.strip() if hasattr(response, 'content') else str(response).strip()


def summarize(state: NewsState) -> NewsState:
    """Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø®Ø¨Ø±Ù‡Ø§ Ø¨Ø§ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† Ú©Ø´"""

    logger.info("Summarizing articles (batch size: %d)", BATCH_SIZE)
    cache = state.get('cache', {})

    for art in state['articles'][:50]:
        content = art['content']
        link = art['link']
        h = art['hash']

        if 'summary' in cache.get(link, {}):
            logger.info(f"Skipping already summarized article: {link}")
            continue

        try:
            summary_text = summarize_article(content)
            cache_entry = cache.setdefault(link, {})
            cache_entry.update({
                'hash': h,
                'summary': summary_text
            })
        except Exception as e:
            logger.error(f"Failed to summarize article {link}: {str(e)}")
            cache_entry = cache.setdefault(link, {})
            cache_entry.update({
                'hash': h,
                'summary': 'Error in summarization'
            })

    state['cache'] = cache
    return state
